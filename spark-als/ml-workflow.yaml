# This workflow will execute spark examples under examples, mllib and ml
#
# To invoke (example):
#
# $ k proxy # if necessary 
# $ argo submit spark-pi.yaml
# Dockerfile for images below are in:
# https://code.devops.fds.com/EMLP/second_opinion/blob/master/argo_spark_2.3.0/Dockerfile
# https://code.devops.fds.com/EMLP/second_opinion/blob/master/spark_2.3.0/Dockerfile
# Tim Fox

apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: als-spark-ml-andrea
spec:
  entrypoint: start
  onExit: exit-handler                  #invoke exit-hander template at end of the workflow

  arguments:
    parameters:
    - name: k8s-host
      value: https://35.192.68.214
    - name: spark-executors-number
      value: 5
    - name: spark-argo-image
      value: ci-artifacts.devops.fds.com/emlp/argo_spark_2.3.0
    - name: spark-image
#      value: ci-artifacts.devops.fds.com/emlp/emlp-spark-2.3.0:1.0.3
      value: ci-artifacts.devops.fds.com/emlp/emlp-spark-2.3.1:1.0.3
    - name: spark-pod-name
      value: als-spark-ml-driver
    - name: script-path
#      value: local:///opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar
      value: local:///opt/spark/examples/jars/spark-examples_2.11-2.3.1.jar
    - name: spark-cluster-name
      value: als-spark-ml
    - name: class-to-execute
      # value: SparkPi # works
      # value: SparkALS # works
      value: ml.ALSExample # works - lists movie ids
      # value: mllib.MovieLensALS # NoClassDefFoundError: scopt/OptionParser -- creating a file assembly.sbt in the project/ directory, add line addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.13.0")
      # value: mllib.RecommendationExample # Total input paths to process : 0 -- RecommendationExample.scala:63 -- MatrixFactorizationModel.scala:337

  templates:
  - name: start
    steps:
    - - name: print-class-to-execute
        template: print-class-to-execute
    - - name: run-spark
        template: spark
        # template: spark-movie-lens
    - - name: get-logs
        template: get-spark-logs
    - - name: delete
        template: delete-spark-pod

  - name: spark
    container:
      image: '{{workflow.parameters.spark-argo-image}}'
      imagePullPolicy: Always
      command: [sh, -c]
        # --class org.apache.spark.examples.mllib.RecommendationExample \
      args: ["bin/spark-submit --master k8s://{{workflow.parameters.k8s-host}} \
        --deploy-mode cluster \
        --name {{workflow.parameters.spark-cluster-name}} \
        --class org.apache.spark.examples.{{workflow.parameters.class-to-execute}} \
        --conf spark.executor.instances={{workflow.parameters.spark-executors-number}} \
        --conf spark.kubernetes.container.image={{workflow.parameters.spark-image}} \
        --conf spark.kubernetes.driver.pod.name={{workflow.parameters.spark-pod-name}} \
        {{workflow.parameters.script-path}}
      "]

  - name: spark-movie-lens
    container:
      image: '{{workflow.parameters.spark-argo-image}}'
      imagePullPolicy: Always
      command: [sh, -c]
      args: ["bin/spark-submit --master k8s://{{workflow.parameters.k8s-host}} \
        --deploy-mode cluster \
        --name {{workflow.parameters.spark-cluster-name}} \
        --class org.apache.spark.examples.{{workflow.parameters.class-to-execute}} \
        --conf spark.executor.instances={{workflow.parameters.spark-executors-number}} \
        --conf spark.kubernetes.container.image={{workflow.parameters.spark-image}} \
        --conf spark.kubernetes.driver.pod.name={{workflow.parameters.spark-pod-name}} \
        {{workflow.parameters.script-path}} \
        --rank 5 --numIterations 20 --lambda 1.0 --kryo
      "]

  - name: get-spark-logs
    container:
      image: argoproj/argoexec:latest
      command: [sh, -c]
      args: ["kubectl logs {{workflow.parameters.spark-pod-name}}"]

  - name: delete-spark-pod
    resource:
      action: delete
      manifest: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: {{workflow.parameters.spark-pod-name}}

  - name: print-class-to-execute
    container:
      image: alpine:latest
      command: [sh, -c]
      args: ["echo Executing class {{workflow.parameters.class-to-execute}}"]

# Exit handler templates
  # After the completion of the entrypoint template, the status of the
  # workflow is made available in the global variable {{workflow.status}}.
  # {{workflow.status}} will be one of: Succeeded, Failed, Error
  - name: exit-handler
    steps:
    - - name: notify
        template: send-email
      - name: wf-ok
        template: success-action
        when: "{{workflow.status}} == Succeeded"
      - name: handle-error
        template: failure-action
        when: "{{workflow.status}} != Succeeded"
  - name: send-email
    container:
      image: alpine:latest
      command: [sh, -c]
      args: ["echo send e-mail: {{workflow.name}} {{workflow.status}}"]
  - name: success-action
    container:
      image: alpine:latest
      command: [sh, -c]
      args: ["echo on to the next workflow"]
  - name: failure-action
    container:
      image: alpine:latest
      command: [sh, -c]
      args: ["echo ERROR: {{workflow.name}} {{workflow.status}}. After correcting the issue, consider repeating the workflow with the retry option"]
